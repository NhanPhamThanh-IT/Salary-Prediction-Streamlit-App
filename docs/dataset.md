# <div align="center">Dataset Learning Materials</div>

<div align="justify">

## Table of Contents
1. [Introduction to Datasets](#introduction-to-datasets)
2. [Data Types and Structures](#data-types-and-structures)
3. [Data Collection and Sources](#data-collection-and-sources)
4. [Data Cleaning and Preprocessing](#data-cleaning-and-preprocessing)
5. [Data Exploration and Analysis](#data-exploration-and-analysis)
6. [Data Visualization](#data-visualization)
7. [Data Storage and Management](#data-storage-and-management)
8. [Machine Learning Datasets](#machine-learning-datasets)
9. [Best Practices](#best-practices)
10. [Tools and Libraries](#tools-and-libraries)
11. [Learning Paths](#learning-paths)
12. [Resources and Community](#resources-and-community)

## Introduction to Datasets

### What is a Dataset?
A dataset is a collection of data, typically organized in a structured format such as a table, that contains information about a specific topic or domain. Datasets are fundamental to data science, machine learning, and statistical analysis.

### Key Characteristics
- **Structured**: Organized in rows and columns (tabular data)
- **Unstructured**: Text, images, audio, video files
- **Semi-structured**: JSON, XML, or other hierarchical formats
- **Size**: Can range from small (few records) to big data (millions/billions of records)
- **Quality**: Varies from clean, well-documented to messy, incomplete data

### Types of Datasets
1. **Numerical Data**: Quantitative measurements
2. **Categorical Data**: Qualitative classifications
3. **Time Series Data**: Data points collected over time
4. **Text Data**: Natural language content
5. **Image Data**: Visual information
6. **Audio Data**: Sound recordings
7. **Geospatial Data**: Location-based information

## Data Types and Structures

### Tabular Data
```python
import pandas as pd

# Sample dataset structure
data = {
    'id': [1, 2, 3, 4, 5],
    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
    'age': [25, 30, 35, 28, 32],
    'salary': [50000, 60000, 75000, 55000, 70000],
    'department': ['IT', 'HR', 'IT', 'Marketing', 'IT']
}

df = pd.DataFrame(data)
print(df)
```

### JSON Data
```python
import json

# JSON dataset structure
json_data = {
    "users": [
        {
            "id": 1,
            "name": "Alice",
            "profile": {
                "age": 25,
                "location": "New York",
                "interests": ["reading", "traveling"]
            }
        },
        {
            "id": 2,
            "name": "Bob",
            "profile": {
                "age": 30,
                "location": "Los Angeles",
                "interests": ["sports", "music"]
            }
        }
    ]
}

# Convert to DataFrame
df = pd.json_normalize(json_data['users'])
```

### Time Series Data
```python
import pandas as pd
from datetime import datetime, timedelta

# Create time series data
dates = pd.date_range('2023-01-01', periods=100, freq='D')
time_series_data = pd.DataFrame({
    'date': dates,
    'value': np.random.randn(100).cumsum(),
    'category': np.random.choice(['A', 'B', 'C'], 100)
})

time_series_data.set_index('date', inplace=True)
```

## Data Collection and Sources

### Public Datasets
```python
# Popular public dataset sources
datasets = {
    "Kaggle": "https://www.kaggle.com/datasets",
    "UCI ML Repository": "https://archive.ics.uci.edu/ml/",
    "Google Dataset Search": "https://datasetsearch.research.google.com/",
    "AWS Open Data": "https://registry.opendata.aws/",
    "Hugging Face": "https://huggingface.co/datasets",
    "OpenML": "https://www.openml.org/"
}
```

### Web Scraping
```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_data(url):
    """Basic web scraping function"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Extract data (example)
    data = []
    for item in soup.find_all('div', class_='item'):
        title = item.find('h2').text.strip()
        price = item.find('span', class_='price').text.strip()
        data.append({'title': title, 'price': price})
    
    return pd.DataFrame(data)

# Usage
# df = scrape_data('https://example.com/products')
```

### API Data Collection
```python
import requests
import pandas as pd

def fetch_api_data(url, params=None):
    """Fetch data from REST API"""
    response = requests.get(url, params=params)
    
    if response.status_code == 200:
        data = response.json()
        return pd.DataFrame(data)
    else:
        raise Exception(f"API request failed: {response.status_code}")

# Example: Fetch weather data
def get_weather_data(city, api_key):
    url = f"http://api.openweathermap.org/data/2.5/weather"
    params = {
        'q': city,
        'appid': api_key,
        'units': 'metric'
    }
    return fetch_api_data(url, params)
```

### Database Connections
```python
import sqlite3
import pandas as pd
from sqlalchemy import create_engine

# SQLite connection
def load_from_sqlite(db_path, query):
    """Load data from SQLite database"""
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(query, conn)
    conn.close()
    return df

# PostgreSQL connection
def load_from_postgres(connection_string, query):
    """Load data from PostgreSQL database"""
    engine = create_engine(connection_string)
    df = pd.read_sql_query(query, engine)
    return df

# Example usage
# df = load_from_sqlite('database.db', 'SELECT * FROM users')
```

## Data Cleaning and Preprocessing

### Handling Missing Values
```python
import pandas as pd
import numpy as np

# Create sample data with missing values
data = {
    'name': ['Alice', 'Bob', None, 'Diana', 'Eve'],
    'age': [25, 30, 35, None, 32],
    'salary': [50000, None, 75000, 55000, 70000],
    'department': ['IT', 'HR', 'IT', None, 'IT']
}

df = pd.DataFrame(data)

# Check for missing values
print("Missing values:")
print(df.isnull().sum())

# Remove rows with missing values
df_clean = df.dropna()

# Fill missing values
df_filled = df.fillna({
    'name': 'Unknown',
    'age': df['age'].mean(),
    'salary': df['salary'].median(),
    'department': 'Unknown'
})

# Forward fill for time series
df_ffill = df.fillna(method='ffill')

# Interpolation for numerical data
df_interpolated = df.interpolate()
```

### Data Type Conversion
```python
# Convert data types
df['age'] = df['age'].astype(int)
df['salary'] = df['salary'].astype(float)
df['department'] = df['department'].astype('category')

# Convert date strings to datetime
df['date'] = pd.to_datetime(df['date_string'])

# Convert categorical to numerical
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['department_encoded'] = le.fit_transform(df['department'])
```

### Outlier Detection and Handling
```python
import numpy as np
from scipy import stats

def detect_outliers_iqr(data, column):
    """Detect outliers using IQR method"""
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers

def detect_outliers_zscore(data, column, threshold=3):
    """Detect outliers using Z-score method"""
    z_scores = np.abs(stats.zscore(data[column]))
    outliers = data[z_scores > threshold]
    return outliers

# Example usage
outliers_iqr = detect_outliers_iqr(df, 'salary')
outliers_zscore = detect_outliers_zscore(df, 'salary')

# Remove outliers
df_no_outliers = df[~df.index.isin(outliers_iqr.index)]
```

### Data Normalization and Standardization
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# Standardization (Z-score normalization)
scaler = StandardScaler()
df['salary_standardized'] = scaler.fit_transform(df[['salary']])

# Min-Max scaling
minmax_scaler = MinMaxScaler()
df['salary_normalized'] = minmax_scaler.fit_transform(df[['salary']])

# Robust scaling (handles outliers better)
robust_scaler = RobustScaler()
df['salary_robust'] = robust_scaler.fit_transform(df[['salary']])
```

## Data Exploration and Analysis

### Basic Statistics
```python
# Descriptive statistics
print("Basic statistics:")
print(df.describe())

# Summary statistics by group
print("\nStatistics by department:")
print(df.groupby('department')['salary'].describe())

# Correlation analysis
correlation_matrix = df[['age', 'salary']].corr()
print("\nCorrelation matrix:")
print(correlation_matrix)
```

### Data Distribution Analysis
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Histogram
plt.figure(figsize=(10, 6))
plt.subplot(2, 2, 1)
plt.hist(df['salary'], bins=20, alpha=0.7)
plt.title('Salary Distribution')
plt.xlabel('Salary')
plt.ylabel('Frequency')

# Box plot
plt.subplot(2, 2, 2)
plt.boxplot(df['salary'])
plt.title('Salary Box Plot')
plt.ylabel('Salary')

# Q-Q plot for normality
plt.subplot(2, 2, 3)
from scipy import stats
stats.probplot(df['salary'], dist="norm", plot=plt)
plt.title('Q-Q Plot')

# Density plot
plt.subplot(2, 2, 4)
df['salary'].plot(kind='density')
plt.title('Salary Density Plot')

plt.tight_layout()
plt.show()
```

### Categorical Data Analysis
```python
# Frequency analysis
department_counts = df['department'].value_counts()
print("Department distribution:")
print(department_counts)

# Cross-tabulation
cross_tab = pd.crosstab(df['department'], df['age_group'])
print("\nCross-tabulation:")
print(cross_tab)

# Chi-square test for independence
from scipy.stats import chi2_contingency
chi2, p_value, dof, expected = chi2_contingency(cross_tab)
print(f"\nChi-square test p-value: {p_value}")
```

## Data Visualization

### Basic Plots with Matplotlib
```python
import matplotlib.pyplot as plt

# Line plot
plt.figure(figsize=(12, 8))
plt.subplot(2, 3, 1)
plt.plot(df['age'], df['salary'], 'o-')
plt.title('Salary vs Age')
plt.xlabel('Age')
plt.ylabel('Salary')

# Scatter plot
plt.subplot(2, 3, 2)
plt.scatter(df['age'], df['salary'], alpha=0.6)
plt.title('Salary vs Age (Scatter)')
plt.xlabel('Age')
plt.ylabel('Salary')

# Bar plot
plt.subplot(2, 3, 3)
department_avg_salary = df.groupby('department')['salary'].mean()
plt.bar(department_avg_salary.index, department_avg_salary.values)
plt.title('Average Salary by Department')
plt.xticks(rotation=45)

# Histogram
plt.subplot(2, 3, 4)
plt.hist(df['salary'], bins=15, alpha=0.7, color='skyblue')
plt.title('Salary Distribution')
plt.xlabel('Salary')
plt.ylabel('Frequency')

# Box plot
plt.subplot(2, 3, 5)
plt.boxplot([df[df['department']==dept]['salary'] for dept in df['department'].unique()])
plt.title('Salary by Department')
plt.xticks(range(1, len(df['department'].unique())+1), df['department'].unique(), rotation=45)

# Pie chart
plt.subplot(2, 3, 6)
plt.pie(department_counts.values, labels=department_counts.index, autopct='%1.1f%%')
plt.title('Department Distribution')

plt.tight_layout()
plt.show()
```

### Advanced Plots with Seaborn
```python
import seaborn as sns

# Set style
sns.set_style("whitegrid")

# Pair plot
sns.pairplot(df[['age', 'salary', 'department']], hue='department')
plt.show()

# Heatmap
plt.figure(figsize=(8, 6))
correlation_matrix = df[['age', 'salary']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

# Violin plot
plt.figure(figsize=(10, 6))
sns.violinplot(x='department', y='salary', data=df)
plt.title('Salary Distribution by Department')
plt.xticks(rotation=45)
plt.show()

# Joint plot
sns.jointplot(x='age', y='salary', data=df, kind='scatter')
plt.show()
```

### Interactive Plots with Plotly
```python
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Interactive scatter plot
fig = px.scatter(df, x='age', y='salary', color='department', 
                 title='Salary vs Age by Department',
                 hover_data=['name'])
fig.show()

# Interactive histogram
fig = px.histogram(df, x='salary', color='department', 
                   title='Salary Distribution by Department',
                   barmode='overlay')
fig.show()

# Interactive box plot
fig = px.box(df, x='department', y='salary', 
             title='Salary Distribution by Department')
fig.show()

# 3D scatter plot
fig = px.scatter_3d(df, x='age', y='salary', z='department',
                    title='3D Scatter Plot')
fig.show()
```

## Data Storage and Management

### File Formats
```python
# CSV files
df.to_csv('data.csv', index=False)
df = pd.read_csv('data.csv')

# Excel files
df.to_excel('data.xlsx', index=False)
df = pd.read_excel('data.xlsx')

# JSON files
df.to_json('data.json', orient='records')
df = pd.read_json('data.json')

# Parquet files (efficient for large datasets)
df.to_parquet('data.parquet', index=False)
df = pd.read_parquet('data.parquet')

# HDF5 files
df.to_hdf('data.h5', key='df', mode='w')
df = pd.read_hdf('data.h5', key='df')
```

### Database Operations
```python
import sqlite3
from sqlalchemy import create_engine

# SQLite operations
def save_to_sqlite(df, db_path, table_name):
    """Save DataFrame to SQLite database"""
    conn = sqlite3.connect(db_path)
    df.to_sql(table_name, conn, if_exists='replace', index=False)
    conn.close()

def load_from_sqlite(db_path, table_name):
    """Load DataFrame from SQLite database"""
    conn = sqlite3.connect(db_path)
    df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    conn.close()
    return df

# PostgreSQL operations
def save_to_postgres(df, connection_string, table_name):
    """Save DataFrame to PostgreSQL database"""
    engine = create_engine(connection_string)
    df.to_sql(table_name, engine, if_exists='replace', index=False)

def load_from_postgres(connection_string, table_name):
    """Load DataFrame from PostgreSQL database"""
    engine = create_engine(connection_string)
    df = pd.read_sql_table(table_name, engine)
    return df
```

### Data Versioning
```python
import hashlib
import json
from datetime import datetime

class DatasetVersion:
    def __init__(self, df, description=""):
        self.data = df
        self.description = description
        self.timestamp = datetime.now()
        self.hash = self._calculate_hash()
    
    def _calculate_hash(self):
        """Calculate hash of the dataset"""
        data_str = self.data.to_string()
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def save_metadata(self, filename):
        """Save dataset metadata"""
        metadata = {
            'description': self.description,
            'timestamp': self.timestamp.isoformat(),
            'hash': self.hash,
            'shape': self.data.shape,
            'columns': list(self.data.columns)
        }
        
        with open(filename, 'w') as f:
            json.dump(metadata, f, indent=2)

# Usage
# version = DatasetVersion(df, "Initial dataset")
# version.save_metadata('dataset_metadata.json')
```

## Machine Learning Datasets

### Dataset Splitting
```python
from sklearn.model_selection import train_test_split

# Basic train-test split
X = df[['age', 'department_encoded']]
y = df['salary']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Stratified split (for classification)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Time series split
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
```

### Feature Engineering
```python
# Create new features
df['age_squared'] = df['age'] ** 2
df['salary_per_age'] = df['salary'] / df['age']

# Binning continuous variables
df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], 
                        labels=['Young', 'Adult', 'Middle-aged', 'Senior'])

# One-hot encoding
df_encoded = pd.get_dummies(df, columns=['department'], prefix='dept')

# Feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
df[['age_scaled', 'salary_scaled']] = scaler.fit_transform(df[['age', 'salary']])
```

### Dataset Validation
```python
def validate_dataset(df, required_columns, data_types, constraints):
    """Validate dataset structure and content"""
    errors = []
    
    # Check required columns
    missing_columns = set(required_columns) - set(df.columns)
    if missing_columns:
        errors.append(f"Missing columns: {missing_columns}")
    
    # Check data types
    for col, expected_type in data_types.items():
        if col in df.columns:
            if not isinstance(df[col].dtype, expected_type):
                errors.append(f"Column {col} has wrong type: {df[col].dtype}")
    
    # Check constraints
    for col, constraint in constraints.items():
        if col in df.columns:
            if constraint['min'] is not None and df[col].min() < constraint['min']:
                errors.append(f"Column {col} has values below minimum")
            if constraint['max'] is not None and df[col].max() > constraint['max']:
                errors.append(f"Column {col} has values above maximum")
    
    return errors

# Example usage
required_columns = ['name', 'age', 'salary']
data_types = {'age': int, 'salary': float}
constraints = {
    'age': {'min': 0, 'max': 120},
    'salary': {'min': 0, 'max': None}
}

validation_errors = validate_dataset(df, required_columns, data_types, constraints)
if validation_errors:
    print("Validation errors:", validation_errors)
else:
    print("Dataset is valid!")
```

## Best Practices

### Data Documentation
```python
class DatasetDocumentation:
    def __init__(self, df, name, description=""):
        self.df = df
        self.name = name
        self.description = description
        self.metadata = {}
    
    def generate_summary(self):
        """Generate comprehensive dataset summary"""
        summary = {
            'name': self.name,
            'description': self.description,
            'shape': self.df.shape,
            'columns': list(self.df.columns),
            'data_types': self.df.dtypes.to_dict(),
            'missing_values': self.df.isnull().sum().to_dict(),
            'unique_values': {col: self.df[col].nunique() for col in self.df.columns},
            'numerical_summary': self.df.describe().to_dict() if self.df.select_dtypes(include=[np.number]).shape[1] > 0 else {},
            'categorical_summary': {col: self.df[col].value_counts().to_dict() 
                                  for col in self.df.select_dtypes(include=['object', 'category']).columns}
        }
        return summary
    
    def save_documentation(self, filename):
        """Save documentation to file"""
        summary = self.generate_summary()
        with open(filename, 'w') as f:
            json.dump(summary, f, indent=2)

# Usage
# doc = DatasetDocumentation(df, "Employee Dataset", "Dataset containing employee information")
# doc.save_documentation('dataset_documentation.json')
```

### Data Quality Assessment
```python
def assess_data_quality(df):
    """Comprehensive data quality assessment"""
    quality_report = {
        'completeness': {},
        'consistency': {},
        'accuracy': {},
        'timeliness': {},
        'validity': {}
    }
    
    # Completeness
    for col in df.columns:
        missing_pct = (df[col].isnull().sum() / len(df)) * 100
        quality_report['completeness'][col] = {
            'missing_count': df[col].isnull().sum(),
            'missing_percentage': missing_pct,
            'completeness_score': 100 - missing_pct
        }
    
    # Consistency
    for col in df.columns:
        if df[col].dtype in ['object', 'category']:
            quality_report['consistency'][col] = {
                'unique_values': df[col].nunique(),
                'most_common': df[col].mode().iloc[0] if not df[col].mode().empty else None
            }
    
    # Validity (basic checks)
    for col in df.columns:
        if df[col].dtype in ['int64', 'float64']:
            quality_report['validity'][col] = {
                'negative_values': (df[col] < 0).sum(),
                'zero_values': (df[col] == 0).sum(),
                'outliers_iqr': len(detect_outliers_iqr(df, col))
            }
    
    return quality_report

# Usage
# quality_report = assess_data_quality(df)
# print(json.dumps(quality_report, indent=2))
```

### Data Pipeline
```python
class DataPipeline:
    def __init__(self):
        self.steps = []
    
    def add_step(self, name, function, **kwargs):
        """Add a processing step to the pipeline"""
        self.steps.append({
            'name': name,
            'function': function,
            'kwargs': kwargs
        })
    
    def run(self, data):
        """Execute all steps in the pipeline"""
        result = data
        for step in self.steps:
            print(f"Executing: {step['name']}")
            result = step['function'](result, **step['kwargs'])
        return result

# Example pipeline
def load_data(file_path):
    return pd.read_csv(file_path)

def clean_data(df):
    return df.dropna()

def encode_categorical(df):
    return pd.get_dummies(df, columns=['department'])

def scale_features(df):
    scaler = StandardScaler()
    df[['age_scaled', 'salary_scaled']] = scaler.fit_transform(df[['age', 'salary']])
    return df

# Create and run pipeline
pipeline = DataPipeline()
pipeline.add_step("Load Data", load_data, file_path='data.csv')
pipeline.add_step("Clean Data", clean_data)
pipeline.add_step("Encode Categorical", encode_categorical)
pipeline.add_step("Scale Features", scale_features)

# processed_data = pipeline.run(None)
```

## Tools and Libraries

### Essential Python Libraries
```python
# Core data manipulation
import pandas as pd
import numpy as np

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go

# Machine learning
from sklearn import preprocessing, model_selection, metrics
import scipy.stats as stats

# Web scraping and APIs
import requests
from bs4 import BeautifulSoup
import json

# Database connections
import sqlite3
from sqlalchemy import create_engine

# Big data
import dask.dataframe as dd
import vaex

# Data validation
import great_expectations as ge
import pandera as pa
```

### Data Quality Tools
```python
# Great Expectations
def create_expectations(df):
    """Create data quality expectations"""
    ge_df = ge.from_pandas(df)
    
    # Column expectations
    ge_df.expect_column_to_exist("age")
    ge_df.expect_column_values_to_be_between("age", 0, 120)
    ge_df.expect_column_values_to_not_be_null("name")
    
    return ge_df

# Pandera
from pandera import DataFrameSchema, Column, Check

schema = DataFrameSchema({
    "name": Column(str, Check.str_length(1, 50)),
    "age": Column(int, Check.in_range(0, 120)),
    "salary": Column(float, Check.greater_than(0)),
    "department": Column(str, Check.isin(["IT", "HR", "Marketing"]))
})

# Validate data
try:
    schema.validate(df)
    print("Data is valid!")
except Exception as e:
    print(f"Validation failed: {e}")
```

### Big Data Tools
```python
# Dask for large datasets
import dask.dataframe as dd

# Read large CSV file
ddf = dd.read_csv('large_file.csv')

# Perform operations
result = ddf.groupby('department')['salary'].mean().compute()

# Vaex for very large datasets
import vaex

# Read large dataset
df_vaex = vaex.read_csv('very_large_file.csv')

# Fast operations
result = df_vaex.groupby('department').agg({'salary': 'mean'})
```

## Learning Paths

### Beginner Path (2-4 weeks)
1. **Week 1**: Data Types and Basic Operations
   - Learn pandas basics
   - Understand different data types
   - Practice with small datasets

2. **Week 2**: Data Cleaning and Preprocessing
   - Handle missing values
   - Remove duplicates
   - Basic data transformations

3. **Week 3**: Data Visualization
   - Matplotlib and Seaborn basics
   - Create basic charts
   - Interpret visualizations

4. **Week 4**: Data Analysis
   - Descriptive statistics
   - Correlation analysis
   - Basic statistical tests

### Intermediate Path (1-2 months)
1. **Advanced Data Manipulation**
   - Complex data transformations
   - Time series data
   - Text data processing

2. **Advanced Visualization**
   - Interactive plots with Plotly
   - Custom visualizations
   - Dashboard creation

3. **Data Quality and Validation**
   - Data quality assessment
   - Automated validation
   - Data profiling

### Advanced Path (2-3 months)
1. **Big Data Processing**
   - Dask and Vaex
   - Distributed computing
   - Performance optimization

2. **Machine Learning Datasets**
   - Feature engineering
   - Dataset splitting strategies
   - Cross-validation

3. **Data Engineering**
   - ETL pipelines
   - Data warehousing
   - Real-time data processing

## Resources and Community

### Official Documentation
- **[Pandas Documentation](https://pandas.pydata.org/docs/)**: Complete pandas guide
- **[NumPy Documentation](https://numpy.org/doc/)**: Numerical computing
- **[Matplotlib Documentation](https://matplotlib.org/)**: Plotting library
- **[Seaborn Documentation](https://seaborn.pydata.org/)**: Statistical visualization

### Online Courses
- **DataCamp**: Python for Data Science
- **Coursera**: Applied Data Science with Python
- **edX**: Data Science Fundamentals
- **Kaggle Learn**: Pandas and Data Visualization

### Books
- **"Python for Data Analysis"** by Wes McKinney
- **"Data Science Handbook"** by Jake VanderPlas
- **"Python Data Science Handbook"** by Jake VanderPlas
- **"Hands-On Machine Learning"** by Aurélien Géron

### Communities
- **[Stack Overflow](https://stackoverflow.com/questions/tagged/pandas)**: Q&A platform
- **[Reddit r/datascience](https://www.reddit.com/r/datascience/)**: Data science community
- **[Kaggle Forums](https://www.kaggle.com/discussions)**: Data science discussions
- **[Data Science Central](https://www.datasciencecentral.com/)**: Professional network

### Datasets for Practice
- **[Kaggle Datasets](https://www.kaggle.com/datasets)**: Wide variety of datasets
- **[UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/)**: Academic datasets
- **[Google Dataset Search](https://datasetsearch.research.google.com/)**: Dataset discovery
- **[AWS Open Data](https://registry.opendata.aws/)**: Large-scale datasets

### Tools and Platforms
- **[Jupyter Notebook](https://jupyter.org/)**: Interactive development
- **[Google Colab](https://colab.research.google.com/)**: Cloud-based notebooks
- **[Databricks](https://databricks.com/)**: Unified analytics platform
- **[Apache Airflow](https://airflow.apache.org/)**: Data pipeline orchestration

---

</div>

<div align="center">

*This learning guide provides a comprehensive introduction to working with datasets in data science. For the latest tools and techniques, always refer to the official documentation and stay updated with the data science community.*

</div>